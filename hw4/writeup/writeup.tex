%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CS484 Written Question Template
%
% Acknowledgements:
% The original code is written by Prof. James Tompkin (james_tompkin@brown.edu).
% The second version is revised by Prof. Min H. Kim (minhkim@kaist.ac.kr).
%
% This is a LaTeX document. LaTeX is a markup language for producing 
% documents. Your task is to fill out this document, then to compile 
% it into a PDF document. 
%
% 
% TO COMPILE:
% > pdflatex thisfile.tex
%
% If you do not have LaTeX and need a LaTeX distribution:
% - Personal laptops (all common OS): www.latex-project.org/get/
% - We recommend latex compiler miktex (https://miktex.org/) for windows,
%   macTex (http://www.tug.org/mactex/) for macOS users.
%   And TeXstudio(http://www.texstudio.org/) for latex editor.
%   You should install both compiler and editor for editing latex.
%   The another option is Overleaf (https://www.overleaf.com/) which is 
%   an online latex editor.
%
% If you need help with LaTeX, please come to office hours. 
% Or, there is plenty of help online:
% https://en.wikibooks.org/wiki/LaTeX
%
% Good luck!
% Min and the CS484 staff
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% How to include two graphics on the same line:
% 
% \includegraphics[width=0.49\linewidth]{yourgraphic1.png}
% \includegraphics[width=0.49\linewidth]{yourgraphic2.png}
%
% How to include equations:
%
% \begin{equation}
% y = mx+c
% \end{equation}
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue]{hyperref}
\usepackage[a4paper,margin=1.5in]{geometry}
\usepackage{stackengine,graphicx}
\usepackage{fancyhdr}
\setlength{\headheight}{15pt}
\usepackage{microtype}
\usepackage{times}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{wrapfig}

% From https://ctan.org/pkg/matlab-prettifier
\usepackage[numbered,framed]{matlab-prettifier}

\frenchspacing
\setlength{\parindent}{0cm} % Default is 15pt.
\setlength{\parskip}{0.3cm plus1mm minus1mm}

\pagestyle{fancy}
\fancyhf{}
\lhead{Project Writeup}
\rhead{CS 484}
\rfoot{\thepage}

\date{}

\title{\vspace{-1cm}Homework 4 Writeup}


\begin{document}
\maketitle
\vspace{-3cm}
\thispagestyle{fancy}

\section*{Instructions}
\begin{itemize}
  \item Describe any interesting decisions you made to write your algorithm.
  \item Show and discuss the results of your algorithm.
  \item Feel free to include code snippets, images, and equations.
  \item Use as many pages as you need, but err on the short side If you feel you only need to write a short amount to meet the brief, th
  
  \item \textbf{Please make this document anonymous.}
\end{itemize}

\section*{A. get$\_$interest$\_$points.m}
For the function get$\_$interest$\_$points, I used a normal Harris corner detection method. I followed the algorithms below to implement Harris corner detection: 
\begin{enumerate}
	\item Compute image derivatives by applying sobel filters to image.
	\item Compute M components as squares of derivatives by finding $I_x^2$, $I_y^2$, and $I_xI_y$.
	\item Apply a Gaussian filter of size $[4,4]$ and width $2$ to each of $I_x^2$, $I_y^2$, and $I_xI_y$.
	\item Compute cornerness $\text{C} = \text{det(M)} - \alpha \text{trace(M)}^2$.
	\item Apply threshold on C to pick high cornerness.
	\item Non-maxima suppression to pick peaks. I used function colfilt() to find maximum peaks.
\end{enumerate}

\section*{B. get$\_$descriptors.m}
\subsection*{Normalized Patches}
When using normalized patches, for Notre Dame, the accuracy on all points was 73.95$\%$ for 215 points, and 83$\%$ for top 100 points. For Mount Rushmore, the accruacy on all points was 84.5$\%$ for 129 submitted poitns, and 92$\%$ for top 100 points. For Gaudi's Episcopal Palace, the accuracy on all points was 12.5$\%$ for 8 points, and 1$\%$ for top 100 points. \\

Figure 1 below show the results obtained for the three images - Notre Dame, Mount Rushmore, and Episcopal Gaudi: 
\begin{figure}[h]
	\caption{Normalized Patches}
	\centering
	\begin{subfigure}{0.4\linewidth}
		\includegraphics[width=\linewidth]{../code/eval_simple_ND.png}
		\caption{Notre Dame}
	\end{subfigure} \\
	% 
	\begin{subfigure}{0.4\linewidth}
		\includegraphics[width=\linewidth]{../code/eval_simple_MR.png}
		\caption{Mount Rushmore}
	\end{subfigure}
	%
	\begin{subfigure}{0.4\linewidth}
		\includegraphics[width=\linewidth]{../code/eval_simple_EG.png}
		\caption{Episcopal Gaudi}
	\end{subfigure}
	%
\end{figure}

\subsection*{Basic SIFT}
This performance generally increased by implementing SIFT method. For Notre Dame, the accuracy on all points was 90.79$\%$ for 228 points, and 100$\%$ for top 100 points. For Mount Rushmore, the accruacy on all points was 98.5$\%$ for 133 submitted poitns, and 99$\%$ for top 100 points. For Gaudi's Episcopal Palace, the performance was worse. The accuracy on all points was 0$\%$ for 2 points, and 0$\%$ for top 100 points. \\

For SIFT, I implemented N x 128 features matrix, where N is the number of feature points. I divided 16 x 16 pixel window into 4 x 4 cells, with each cell consisting of 8-directional histogram. After finding features descriptors, I normalized each of the descriptor. Then, I capped all magnitudes higher than 0.2 to 0.2, after which I normalized the descriptor again. \\

Figure 2 below show the results obtained for the three images - Notre Dame, Mount Rushmore, and Episcopal Gaudi: 
\begin{figure}[h]
	\caption{Basic SIFT Implemented}
	\centering
	\begin{subfigure}{0.4\linewidth}
		\includegraphics[width=\linewidth]{../code/eval_sift_ND.png}
		\caption{Notre Dame}
	\end{subfigure} \\
	% 
	\begin{subfigure}{0.4\linewidth}
		\includegraphics[width=\linewidth]{../code/eval_sift_MR.png}
		\caption{Mount Rushmore}
	\end{subfigure}
	%
	\begin{subfigure}{0.4\linewidth}
		\includegraphics[width=\linewidth]{../code/eval_sift_EG.png}
		\caption{Episcopal Gaudi}
	\end{subfigure}
	%
\end{figure}

\pagebreak
\subsection*{Feature Orientation}
I implemented estimation of feature orientation as an extra credit. The performance, however, generally fell with this implementation. For Notre Dame, the accuracy on all points was 79.7$\%$ for 202 points, and 86$\%$ for top 100 points. For Mount Rushmore, the accruacy on all points was 94.32$\%$ for 88 submitted poitns, and 83$\%$ for top 100 points. For Gaudi's Episcopal Palace, the accuracy on all points was 0$\%$ for 2 points, and 0$\%$ for top 100 points. \\

I implemented feature orientation by extracting a window of width 16 around the feature point. Then, I created a histogram of 8 directions of the 256 pixel points to find the direction of highest magnitude. I set this value as max$\_$theta, and I subtracted max$\_$theta from all subsequent theta values obtained when creating histograms for 4 x 4 cells for each feature point. \\

The performance generally fell, but still kept reasonable accuracy. I thought this was so because the test images given are not really oriented to take feature orientation into account. Thus, subtracting max$\_$theta could have actually skewed the features, resulting in drop of accuracy. However, feature descriptors still gave reasonable results.\\

Figure 3 below show the results obtained for the three images - Notre Dame, Mount Rushmore, and Episcopal Gaudi:
\begin{figure}[h]
	\caption{Feature Orientation Implemented}
	\centering
	\begin{subfigure}{0.4\linewidth}
		\includegraphics[width=\linewidth]{../code/eval_orientation_ND.png}
		\caption{Notre Dame}
	\end{subfigure} \\
	% 
	\begin{subfigure}{0.4\linewidth}
		\includegraphics[width=\linewidth]{../code/eval_orientation_MR.png}
		\caption{Mount Rushmore}
	\end{subfigure}
	%
	\begin{subfigure}{0.4\linewidth}
		\includegraphics[width=\linewidth]{../code/eval_orientation_EG.png}
		\caption{Episcopal Gaudi}
	\end{subfigure}
	%
\end{figure}

Two tables shown below compare performance of each method. Table 1 shows the accuracy for 100 points, and Table 2 shows the accuracy for all points. 
\begin{table}[h]
	\caption{Accuracy for 100 points}
	\begin{tabular}{@{}llll@{}}
		& Notre Dame & Mount Rushmore & Episcopal Gaudi \\
		Normalized Patch    & 83\%       & 92\%           & 1\%             \\
		Basic SIFT          & 100\%      & 98.5\%         & 0\%             \\
		Feature Orientation & 86\%       & 83\%           & 0\%            
	\end{tabular}
\end{table}

\begin{table}[h]
	\caption{Accuracy for all points}
	\begin{tabular}{@{}llll@{}}
		& Notre Dame & Mount Rushmore & Episcopal Gaudi \\
		Normalized Patch    & 73.95\%    & 84.5\%        & 12.5\%           \\
		Basic SIFT          & 90.79\%    & 98.5\%        & 0\%              \\
		Feature Orientation & 79.7\%     & 94.32\%       & 0\%            
	\end{tabular}
\end{table}

\section*{C. match$\_$features.m}
I used NNDR test to implement feature matching. I calculated Euclidean distance between all possible combinations of feature points from the two images. Then, I sorted the matrix to find smallest distance and the next smallest distance. Taking quotient of those two numbers gave me NNDR, which I used to determine if that feature point is a valid match by comparing the ratio with the threshold. 

\end{document}
